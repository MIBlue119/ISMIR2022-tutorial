# References - 1st hour



## Slide 1-25

1. Dhariwal, P., & Nichol, A. (2021). Diffusion models beat gans on image synthesis. *Advances in Neural Information Processing Systems*, *34*, 8780-8794.
2. Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., & Sutskever, I. (2020). Jukebox: A generative model for music. *arXiv preprint arXiv:2005.00341*.
3. Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. *arXiv preprint arXiv:1609.03499*.
4. Karras, T., Laine, S., & Aila, T. (2019). A style-based generator architecture for generative adversarial networks. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition* (pp. 4401-4410).
5. https://dreamtonics.com/en/synthesizerv/
6. https://github.com/CompVis/stable-diffusion
7. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2022). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. *arXiv preprint arXiv:2208.12242*.
8. https://runwayml.com/
9. Louie, R., Cohen, A., Huang, C. Z. A., Terry, M., & Cai, C. J. (2020). Cococo: AI-Steering Tools for Music Novices Co-Creating with Generative Models. In HAI-GEN+ user2agent@ IUI.
10. https://www.youtube.com/watch?v=vnRrGCB04WE
11. Llano, M. T., d'Inverno, M., Yee-King, M., McCormack, J., Ilsar, A., Pease, A., & Colton, S. (2022). Explainable computational creativity. *arXiv preprint arXiv:2205.05682*.



## Slide 25-46

12. Oord, Aaron van den, et al. "Wavenet: A generative model for raw audio." *arXiv preprint arXiv:1609.03499* (2016).
13. Wang, Yuxuan, et al. "Tacotron: Towards end-to-end speech synthesis." *arXiv preprint arXiv:1703.10135* (2017).
14. Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. "Learning structured output representation using deep conditional generative models." *Advances in neural information processing systems* 28 (2015).
15. Mirza, Mehdi, and Simon Osindero. "Conditional generative adversarial nets." *arXiv preprint arXiv:1411.1784* (2014).
16. Odena, Augustus, Christopher Olah, and Jonathon Shlens. "Conditional image synthesis with auxiliary classifier gans." *International conference on machine learning*. PMLR, 2017.
17. Miyato, Takeru, and Masanori Koyama. "cGANs with projection discriminator." *arXiv preprint arXiv:1802.05637* (2018).
18. Arjovsky, Martin, Soumith Chintala, and Léon Bottou. "Wasserstein generative adversarial networks." *International conference on machine learning*. PMLR, 2017.
19. Miyato, Takeru, et al. "Spectral normalization for generative adversarial networks." *arXiv preprint arXiv:1802.05957* (2018).
20. Rezende, Danilo, and Shakir Mohamed. "Variational inference with normalizing flows." *International conference on machine learning*. PMLR, 2015.
21. Kingma, Durk P., and Prafulla Dhariwal. "Glow: Generative flow with invertible 1x1 convolutions." *Advances in neural information processing systems* 31 (2018).
22. Rombach, Robin, Patrick Esser, and Bjorn Ommer. "Network-to-network translation with conditional invertible neural networks." *Advances in Neural Information Processing Systems* 33 (2020): 2784-2797.
23. Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." *Advances in Neural Information Processing Systems* 33 (2020): 6840-6851.
24. Ho, Jonathan, and Tim Salimans. "Classifier-free diffusion guidance." *arXiv preprint arXiv:2207.12598* (2022).



## Slide 47-60

25. Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi, M., Eck, D., & Simonyan, K. (2017, July). Neural audio synthesis of musical notes with wavenet autoencoders. In *International Conference on Machine Learning* (pp. 1068-1077). PMLR.
26. Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., & Roberts, A. (2019). Gansynth: Adversarial neural audio synthesis. *arXiv preprint arXiv:1902.08710*.
27. Pascual, S., Bhattacharya, G., Yeh, C., Pons, J., & Serrà, J. (2022). Full-band General Audio Synthesis with Score-based Diffusion. *arXiv preprint arXiv:2210.14661*.
28. Dong, H. W., Zhou, C., Berg-Kirkpatrick, T., & McAuley, J. (2022, May). Deep Performer: Score-to-Audio Music Performance Synthesis. In *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)* (pp. 951-955). IEEE.
29. Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y., & Yu, D. (2022). Diffsound: Discrete diffusion model for text-to-sound generation. *arXiv preprint arXiv:2207.09983*.
30. Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., Défossez, A., Copet, J., ... & Adi, Y. (2022). Audiogen: Textually guided audio generation. *arXiv preprint arXiv:2209.15352*.
31. Engel, J., Hantrakul, L., Gu, C., & Roberts, A. (2020). DDSP: Differentiable digital signal processing. *arXiv preprint arXiv:2001.04643*.
32. Wu, Y., Manilow, E., Deng, Y., Swavely, R., Kastner, K., Cooijmans, T., ... & Engel, J. (2021). MIDI-DDSP: Detailed control of musical performance via hierarchical modeling. *arXiv preprint arXiv:2112.09312*.
33. Steinmetz, C. J., Bryan, N. J., & Reiss, J. D. (2022). Style transfer of audio effects with differentiable signal processing. *arXiv preprint arXiv:2207.08759*.
34. Mor, N., Wolf, L., Polyak, A., & Taigman, Y. (2018). A universal music translation network. *arXiv preprint arXiv:1805.07848*.
35. Choi, H. S., Yang, J., Lee, J., & Kim, H. (2022). NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis. *arXiv preprint arXiv:2211.09407*.
36. Sulun, S., Davies, M. E., & Viana, P. (2022). Symbolic music generation conditioned on continuous-valued emotions. *IEEE Access*, *10*, 44617-44626.
37. https://openai.com/blog/musenet/
38. Yu, Y., Srivastava, A., & Canales, S. (2021). Conditional lstm-gan for melody generation from lyrics. *ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)*, *17*(1), 1-20.
39. Harris, Z., Clarke, L. A., Gagliano, P., Camarena, D., Siddiqui, M., & Castro, P. S. (2021). Malakai: Music That Adapts to the Shape of Emotions. arXiv preprint arXiv:2112.02070.
40. Dai, S., Jin, Z., Gomes, C., & Dannenberg, R. B. (2021). Controllable deep melody generation via hierarchical music structure representation. arXiv preprint arXiv:2109.00663.
41. Young, H., Dumoulin, V., Castro, P. S., Engel, J., & Huang, C. Z. A. (2022). Compositional Steering of Music Transformers.
42. Ens, J., & Pasquier, P. (2020). Mmm: Exploring conditional multi-track music generation with the transformer. arXiv preprint arXiv:2008.06048.
43. Tan, H. H., & Herremans, D. (2020). Music fadernets: Controllable music generation based on high-level features via low-level feature modelling. *arXiv preprint arXiv:2007.15474*.
44. Roberts, A., Engel, J., Raffel, C., Hawthorne, C., & Eck, D. (2018, July). A hierarchical latent vector model for learning long-term structure in music. In International conference on machine learning (pp. 4364-4373). PMLR.
45. Louie, R., Cohen, A., Huang, C. Z. A., Terry, M., & Cai, C. J. (2020). Cococo: AI-Steering Tools for Music Novices Co-Creating with Generative Models. In HAI-GEN+ user2agent@ IUI.
46. Wang, Z., Wang, D., Zhang, Y., & Xia, G. (2020). Learning interpretable representation for controllable polyphonic music generation. *arXiv preprint arXiv:2008.07122*.

## Slide 61-67
47. Streijl, R. C., Winkler, S., & Hands, D. S. (2016). Mean opinion score (MOS) revisited: methods and applications, limitations and alternatives. *Multimedia Systems*, *22* (2), 213-227.
48. https://en.wikipedia.org/wiki/Mean_opinion_score
49. Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., & Roberts, A. (2019). Gansynth: Adversarial neural audio synthesis. *arXiv preprint arXiv:1902.08710*.
50. Nistal, J., Lattner, S., & Richard, G. (2020). Drumgan: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks. *arXiv preprint arXiv:2008.12073*.
51. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., & Chen, X. (2016). Improved techniques for training gans. *Advances in neural information processing systems*, *29*.
52. Kilgour, K., Zuluaga, M., Roblek, D., & Sharifi, M. (2018). Fr\'echet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms. *arXiv preprint arXiv:1812.08466*.
53. Bińkowski, M., Sutherland, D. J., Arbel, M., & Gretton, A. (2018). Demystifying mmd gans. *arXiv preprint arXiv:1801.01401*.
54. Luce, R. D., & Edwards, W. (1958). The derivation of subjective scales from just noticeable differences. *Psychological review*, *65*(4), 222.
55. Wu, Y., Manilow, E., Deng, Y., Swavely, R., Kastner, K., Cooijmans, T., ... & Engel, J. (2021). MIDI-DDSP: Detailed control of musical performance via hierarchical modeling. *arXiv preprint arXiv:2112.09312*.
56. Nistal, J., Lattner, S., & Richard, G. (2020). Drumgan: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks. *arXiv preprint arXiv:2008.12073*.



## Slide 68-73
57. Norman, D. (2013). The design of everyday things: Revised and expanded edition. Basic books.
58. Preece, J., Rogers, Y., Sharp, H., Benyon, D., Holland, S., & Carey, T. (1994). *Human-computer interaction*. Addison-Wesley Longman Ltd..
59. Thelle, N. J., & Pasquier, P. (2021, April). Spire Muse: A Virtual Musical Partner for Creative Brainstorming. In NIME 2021. PubPub.
60. https://ismir2021.ismir.net/tutorials/#2-designing-generative-models-for-interactive-co-creation, https://www.youtube.com/watch?v=f0XO4A_-EeY
61. Molnar, C. (2020). *Interpretable machine learning*. Lulu. com.
62. Gou, J., Yu, B., Maybank, S. J., & Tao, D. (2021). Knowledge distillation: A survey. International Journal of Computer Vision, 129(6), 1789-1819.
63. https://magenta.tensorflow.org/ddsp-vst
64. Garcia, H. F., Aguilar, A., Manilow, E., Vedenko, D., & Pardo, B. (2021). Deep Learning Tools for Audacity: Helping Researchers Expand the Artist's Toolkit. arXiv preprint arXiv:2110.13323.
65. https://github.com/chrisdonahue/music-cocreation-tutorial, https://magenta.tensorflow.org/js-announce, https://tonejs.github.io/, https://webmidijs.org/, https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API

